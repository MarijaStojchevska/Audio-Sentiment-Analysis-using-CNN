{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can be found [here](http://emodb.bilderbar.info/index-1280.html).\n",
    "\n",
    "It contains samples of emotional speech in German, labeled with one of 7 different emotions: Anger, Boredom, Disgust, Fear, Happiness, Sadness and Neutral. \n",
    "\n",
    "Please download the full database and refer to the documentation to understand how the samples are labeled (see \"Additional information\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import urllib.request\n",
    "from flask import Flask\n",
    "import zipfile36 as zipfile\n",
    "from joblib import dump, load\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization, Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates parser and defines arguments \n",
    "def flags():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--dataset_url',\n",
    "        type=str,\n",
    "        default='http://emodb.bilderbar.info/download/download.zip',\n",
    "        help='URL to access the dataset')\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--source_path',\n",
    "        type=str,\n",
    "        default='./',\n",
    "        help='path to the source directory')\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--directory_name',\n",
    "        type=str,\n",
    "        default='emodb.zip',\n",
    "        help='names the downloaded data directory')\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--features_type',\n",
    "        type=str,\n",
    "        default='mfcc',\n",
    "        help='training features [melspec, mfcc]')\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--test_split',\n",
    "        type=float,\n",
    "        default=0.15,\n",
    "        help='split percentages fot the test set')\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--val_split',\n",
    "        type=float,\n",
    "        default=0.15,\n",
    "        help='split percentages fot the validation set')\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--learning_rate',\n",
    "        type=float,\n",
    "        default=1e-3,\n",
    "        help='learning rate')\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--batch_size',\n",
    "        type=int,\n",
    "        default=32,\n",
    "        help='size of the training batch')\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--num_epochs',\n",
    "        type=int,\n",
    "        default=200,\n",
    "        help='number of training epochs')\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--patience',\n",
    "        type=int,\n",
    "        default=20,\n",
    "        help='early stopping patience')\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--port_number',\n",
    "        type=int,\n",
    "        default=105,\n",
    "        help='Flask API port')\n",
    "\n",
    "        \n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "    return FLAGS, unparsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering & Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract(source, src_dir, dir_name):\n",
    "    # Download directory from url\n",
    "    urllib.request.urlretrieve(source, dir_name)\n",
    "\n",
    "    # Extract downloaded dataset directory\n",
    "    source_zip = src_dir+dir_name\n",
    "    dataset_dir = dir_name.split('.')[0]\n",
    "    with zipfile.ZipFile(source_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_dir)\n",
    "    return dataset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_reading(src_dir, dataset_dir):\n",
    "    audio_files = sorted(os.listdir(os.path.join(src_dir+dataset_dir, 'wav')))\n",
    "    audio_directory = src_dir+dataset_dir+'/wav/'\n",
    "    description = []\n",
    "    for f in audio_files:\n",
    "        description.append(f.split('.')[0])\n",
    "\n",
    "    char_to_emotion = {\n",
    "        'W': 'anger',\n",
    "        'L': 'boredom',\n",
    "        'E': 'disgust',\n",
    "        'A': 'fear',\n",
    "        'F': 'happiness',\n",
    "        'T': 'sadness',\n",
    "        'N': 'neutral',\n",
    "    }\n",
    "    emotions = []\n",
    "    for d in description:\n",
    "        emotions.append(char_to_emotion[d[5]])\n",
    "\n",
    "    emotion_data = {\"emotions\": emotions}\n",
    "    emotion_df = pd.DataFrame(emotion_data, index = audio_files)\n",
    "    return audio_files, audio_directory, emotion_df, emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_histogram(emotion_df, emotions):\n",
    "    # Count emotion occurrences\n",
    "    print(emotion_df.emotions.value_counts())\n",
    "    \n",
    "    # Plot histogram of emotion occurrences\n",
    "    plt.title('Emotion occurrences', size=16)\n",
    "    sns.countplot(x=emotions)\n",
    "    plt.xlabel('Emotion')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_padding_or_cut(sound, avgLen):\n",
    "    samples, sr = librosa.load(sound) \n",
    "    lenDifference = abs(len(samples)-avgLen)\n",
    "    cutSize = addSize = int(lenDifference/2)\n",
    "    \n",
    "    if len(samples) < avgLen:\n",
    "        # Padding with 0s\n",
    "        zeros_front = [0] * addSize\n",
    "        if lenDifference % 2 == 0:\n",
    "            zeros_back = [0] * addSize\n",
    "        else:\n",
    "            zeros_back = [0] * (addSize+1)\n",
    "        samples = np.concatenate((zeros_front, samples, zeros_back), axis=0)  \n",
    "        \n",
    "    elif len(samples) > avgLen:\n",
    "        # Length cutting\n",
    "        remaining = samples[cutSize:]\n",
    "        if lenDifference % 2 == 0:\n",
    "            samples = remaining[:len(remaining)-cutSize]\n",
    "        else:\n",
    "            samples = remaining[:len(remaining)-cutSize-1]\n",
    "    return samples, sr\n",
    "\n",
    "def audio_to_melspec(samples, sr):\n",
    "    # Plot raw audio file\n",
    "    pd.Series(samples).plot(figsize=(10, 5))\n",
    "    plt.xlabel('Time Domain')\n",
    "    plt.ylabel('Amplitude')\n",
    "    #plt.show()\n",
    "    # Create audio spectogram\n",
    "    spectogram = librosa.stft(samples)\n",
    "    # Represent frequency on a mel scale\n",
    "    mag, phase = librosa.magphase(spectogram)\n",
    "    mel_scaled_spec = librosa.feature.melspectrogram(S=mag, sr=sr)\n",
    "    # Represent amplitude on a decibel scale\n",
    "    decibel_scaled_spec = librosa.amplitude_to_db(S=mel_scaled_spec, ref=np.min)  \n",
    "    return decibel_scaled_spec\n",
    "\n",
    "def avg_audio_length(audio_files, audio_directory):\n",
    "    sound_lengths = []\n",
    "    for f in audio_files:\n",
    "        sound = audio_directory+f\n",
    "        samples, _ = librosa.load(sound)\n",
    "        sound_lengths.append(len(samples))\n",
    "    avgLen = int(np.mean(sound_lengths))\n",
    "    return avgLen\n",
    "\n",
    "def extract_melspec(samples, sr):\n",
    "    melspec_val = np.mean(audio_to_melspec(samples, sr).T, axis=0)\n",
    "    return melspec_val\n",
    "\n",
    "def extract_mfcc(samples, sr):\n",
    "    mfcc_val = np.mean(librosa.feature.mfcc(y=samples, sr=sr, n_mfcc=40).T, axis=0)\n",
    "    return mfcc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(audio_files, audio_directory, features_type):\n",
    "    data=[]\n",
    "    avgLen = avg_audio_length(audio_files, audio_directory)\n",
    "    \n",
    "    for f in audio_files:\n",
    "        sound = audio_directory+f\n",
    "                \n",
    "        # Length transformation\n",
    "        samples, sr = zero_padding_or_cut(sound, avgLen)\n",
    "        \n",
    "        # Feature extraction\n",
    "        if features_type == 'mfcc':\n",
    "            values = extract_mfcc(samples, sr)\n",
    "        else:\n",
    "            values = extract_melspec(samples, sr)\n",
    "        \n",
    "        data.append(values)\n",
    "    return data\n",
    "\n",
    "def labels_generator(emotion_df):\n",
    "    labels = emotion_df['emotions'].values\n",
    "    encoder = OneHotEncoder()\n",
    "    labels = encoder.fit_transform(np.array(labels).reshape(-1,1)).toarray()\n",
    "    return labels, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepate dataset for deep-learning\n",
    "def dataset_split(audio_files, audio_directory, features_type, emotion_df, test_split, val_split):\n",
    "    data = data_generator(audio_files, audio_directory, features_type)\n",
    "    data = pd.DataFrame(data).iloc[:,:].values\n",
    "    labels, encoder = labels_generator(emotion_df)\n",
    "    print(labels.shape, data.shape)\n",
    "\n",
    "    data, X_test, labels, Y_test = train_test_split(data, labels, test_size=test_split, stratify=labels, shuffle = True)\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(data, labels, test_size=val_split, stratify=labels, shuffle=True)\n",
    "    print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape, X_val.shape, Y_val.shape)\n",
    "\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "    X_val = np.expand_dims(X_val, axis=2)\n",
    "    return X_train, X_test, Y_train, Y_test, X_val, Y_val, encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN(X_train, Y_train, X_val, Y_val, batch_size, learning_rate, num_epochs, patience):\n",
    "    # Define model type\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Define layers\n",
    "    model.add(Conv1D(filters=64, kernel_size=5, padding=\"same\",activation=\"relu\", input_shape=(X_train.shape[1],1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv1D(filters=125, kernel_size=10, padding=\"same\",activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv1D(filters=64, kernel_size=5, padding=\"same\",activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(7, activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(7, activation=\"softmax\"))\n",
    "    \n",
    "    # Model configuration\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    # Model training\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=patience)\n",
    "    history = model.fit(x=X_train, y=Y_train, batch_size=batch_size, epochs=num_epochs, validation_data=(X_val, Y_val), callbacks=[early_stopping], verbose=2)\n",
    "    return model, history\n",
    "\n",
    "def evaluate(model, history, encoder, X_train, X_test, Y_train, Y_test, X_val, Y_val):\n",
    "    # Model evaluation\n",
    "    _, train_accuracy = model.evaluate(X_train, Y_train)\n",
    "    _, val_accuracy = model.evaluate(X_val, Y_val)\n",
    "    _, test_accuracy = model.evaluate(X_test, Y_test)\n",
    "    \n",
    "    print('Train accuracy: %.3f, Validation accuracy: %.3f, Test accuracy: %.3f'  % (train_accuracy, val_accuracy, test_accuracy))\n",
    "    # Plot training and validation loss\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='validation')\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot training and validation accuracy\n",
    "    plt.plot(history.history['accuracy'], label='train')\n",
    "    plt.plot(history.history['val_accuracy'], label='validation')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Labels prediction\n",
    "    predicted_values = model.predict(X_test)\n",
    "    Y_pred = encoder.inverse_transform(predicted_values)\n",
    "    Y_test = encoder.inverse_transform(Y_test)\n",
    "    print(classification_report(Y_test, Y_pred))\n",
    "    \n",
    "def save_model(model, src_dir):\n",
    "    # Save last trained model\n",
    "    model.save(src_dir+'my_model')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flask API endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:105\n",
      " * Running on http://172.18.0.2:105\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Get arguments\n",
    "FLAGS, unparsed = flags()\n",
    "source = FLAGS.dataset_url\n",
    "src_dir = FLAGS.source_path\n",
    "dir_name = FLAGS.directory_name\n",
    "features_type = FLAGS.features_type\n",
    "test_split = FLAGS.test_split\n",
    "val_split = FLAGS.val_split\n",
    "batch_size = FLAGS.batch_size\n",
    "learning_rate = FLAGS.learning_rate\n",
    "num_epochs = FLAGS.num_epochs\n",
    "patience = FLAGS.patience\n",
    "port_number = FLAGS.port_number\n",
    "    \n",
    "app = Flask(__name__)\n",
    "    \n",
    "# Endpoint for model training. Enter 1 in the url variable section to train the model\n",
    "@app.route('/<int:train>/')\n",
    "def model_training(train):\n",
    "    if train==1:\n",
    "        dataset_dir = download_and_extract(source, src_dir, dir_name)\n",
    "        audio_files, audio_directory, emotion_df, emotions = dataset_reading(src_dir, dataset_dir)\n",
    "        class_histogram(emotion_df, emotions)\n",
    "        X_train, X_test, Y_train, Y_test, X_val, Y_val, encoder = dataset_split(audio_files, audio_directory, features_type, emotion_df, test_split, val_split)\n",
    "        dump(encoder, 'encoder.joblib')\n",
    "        model, history = CNN(X_train, Y_train, X_val, Y_val, batch_size, learning_rate, num_epochs, patience)\n",
    "        evaluate(model, history, encoder, X_train, X_test, Y_train, Y_test, X_val, Y_val)\n",
    "        save_model(model, src_dir)\n",
    "        return \"The newly trained model is saved in the my_model directory.\"\n",
    "    else:\n",
    "        return \"To train the model type enter 1 at the end of the url. If the model is already trained, in the url variable section, you can enter the name of the audio file for which you want to predict the emotion.\"\n",
    "\n",
    "    \n",
    "# Endpoint for querying the last trained model with an audio file of our choice. Enter the name of the audio file of interest in the url variable section\n",
    "@app.route('/<string:name>/')\n",
    "def emotion_prediction(name):\n",
    "    \n",
    "    model = tf.keras.models.load_model(src_dir+'my_model')\n",
    "    encoder = load('encoder.joblib') \n",
    "\n",
    "    sound = src_dir+dir_name.split('.')[0]+'/wav/'+name\n",
    "    samples, sr = librosa.load(sound)\n",
    "\n",
    "    if features_type == 'mfcc':\n",
    "        values = extract_mfcc(samples, sr)\n",
    "    else:\n",
    "        values = extract_melspec(samples, sr)\n",
    "\n",
    "    x = pd.DataFrame([values]).iloc[:,:].values\n",
    "    x = np.expand_dims(x, axis=2)\n",
    "\n",
    "    # Predict emotion for one audio sample\n",
    "    prediction = model.predict(x)\n",
    "    predClass = encoder.inverse_transform(prediction)\n",
    "    return \"Predicted emotion for the audio file \" + name + \" is: \" + predClass[0][0]\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='0.0.0.0', port=port_number)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
